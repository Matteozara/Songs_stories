{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project NUANS (EAI) bold text"
      ],
      "metadata": {
        "id": "OS69biVxVlJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the code to run the project.\n",
        "In the following there are some options, like choose the best song among the available ones (link: https://drive.google.com/drive/folders/1LfL88UZII-JQziAx7Y5EO5rDXQOsA-dh?usp=sharing).\n",
        "Then is possible to choose also the gpt2 model between the finetuned ones (link: https://drive.google.com/drive/folders/1umjDaVmtqEQ9LsxcdbvdYHwn1ZtLQFdt?usp=sharing)"
      ],
      "metadata": {
        "id": "zUfTFKPvoniP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and installation"
      ],
      "metadata": {
        "id": "wAfQLg01EWLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert spacy transformers textstat"
      ],
      "metadata": {
        "id": "vgzXEY81ts1q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37d7a80d-c8ef-4149-c691-b3b636d186e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keybert\n",
            "  Downloading keybert-0.7.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.4)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers>=0.3.8 (from keybert)\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.22.4)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.4.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.14.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.2.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.15.2+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (16.0.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (9.4.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Building wheels for collected packages: keybert, sentence-transformers\n",
            "  Building wheel for keybert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keybert: filename=keybert-0.7.0-py3-none-any.whl size=23776 sha256=c0128fccdb32447fcc0101e0d0bfdce6b4957fa08df711ecad0a1e968661e067\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/8d/e6/b0e2f8d883b0fd51819226f67ad9843e04913ce4a97241ff4b\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=314846297f59377a5664480c983ba549c7545f55786f15b8a475bd36019d6f80\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built keybert sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, pyphen, textstat, huggingface-hub, transformers, sentence-transformers, keybert\n",
            "Successfully installed huggingface-hub-0.16.4 keybert-0.7.0 pyphen-0.14.0 safetensors-0.3.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 textstat-0.7.3 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download gpt2 finetuned\n",
        "#!gdown 100qIeatL_KVBbJJjaUgCpBig63-3bPQP #gpt2 2 epochs QAstory dataset\n",
        "#!gdown 1916Fcsq7JRVE7oaM0VWxwgo7nGYLfztU #gpt2 1 epoch TinyStories dataset 40000\n",
        "!gdown 1nwtLMmtX0wAcRW9AUjIW1g1E9FRaUmJa #gpt2 1 epoch TinyStories dataset 80000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHoYgjU0mw-y",
        "outputId": "f8640648-d538-4f29-db24-5604c6f06972"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=100qIeatL_KVBbJJjaUgCpBig63-3bPQP\n",
            "To: /content/story_gpt2_2epochs.pt\n",
            "100% 510M/510M [00:11<00:00, 42.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download t5 finetuned\n",
        "!gdown 1h4GaCcLFa7W3OWHHg6URCY3T3FZtClwh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9LSCMrcnEw_",
        "outputId": "d05015bc-a6ec-4d50-e192-05428e134819"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1h4GaCcLFa7W3OWHHg6URCY3T3FZtClwh\n",
            "To: /content/t5_gc.pt\n",
            "100% 308M/308M [00:05<00:00, 51.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download lyrics file\n",
        "#!gdown 1kjwrOpypgKNh4CsO_5XSKP2iYHhpp95R #without me Eminem\n",
        "#!gdown 1bCwVBKxnAc_rjTSb_GVzozxtYhDiUbTt #moves like jagger Maroon 5\n",
        "#!gdown 1GKtJYQbM5GzOWB97T0gfkb7_rof9Dd8U #levitating dualipa\n",
        "#!gdown 1y8jTTkoQH0CXLFK_Mwt3MNQHmht3__I0 #party in the usa\n",
        "#!gdown 18NfqljJRCBg34c1rTPKBBZBfofiKeG9n #flowers\n",
        "!gdown 1J6iV2RtAMhAcnIoKlbw-QJw0mzV_1erW #perfect\n",
        "#!gdown 1JFrGKSFVD9Y0P5T8IH9QPqTfy1Rbg_Ro #empire state of mind"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu1mu3tYnWlX",
        "outputId": "00e08228-8ee0-4e13-af96-ccb981448a3f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1y8jTTkoQH0CXLFK_Mwt3MNQHmht3__I0\n",
            "To: /content/party_in_the_usa.txt\n",
            "\r  0% 0.00/2.32k [00:00<?, ?B/s]\r100% 2.32k/2.32k [00:00<00:00, 8.71MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import torch\n",
        "from keybert import KeyBERT\n",
        "import spacy\n",
        "from transformers import pipeline, set_seed\n",
        "import re\n",
        "from transformers import T5Tokenizer\n",
        "from transformers import GPT2Tokenizer#, GPT2LMHeadModel, GPTNeoForCausalLM\n",
        "import textstat\n",
        "import nltk\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import Laplace"
      ],
      "metadata": {
        "id": "SnvJx-lSgmUb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jtZs2_Egxvw",
        "outputId": "526b2415-ba0a-47c4-8180-1e1a17f3fca5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ],
      "metadata": {
        "id": "ptc-JkF7EcOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#elaborate ner results separating date from the others and ordering them (dates are ordered by length and the other by recurrency)\n",
        "def elaborate_ner_results(ner_result):\n",
        "  entities = {}\n",
        "  time = []\n",
        "  time_word = \"\"\n",
        "  max = -1\n",
        "  people = []\n",
        "\n",
        "  #count entities and separate \"date\" entities\n",
        "  for ent in ner_result.ents:\n",
        "    if ent.label_ == \"DATE\":\n",
        "      time.append(ent.text)\n",
        "    else:\n",
        "      if ent.text not in entities:\n",
        "        entities[ent.text] = 1\n",
        "      else:\n",
        "        entities[ent.text] += 1\n",
        "\n",
        "  #select most long time\n",
        "  for i in time:\n",
        "    if len(i) > max:\n",
        "      max = len(i)\n",
        "      time_word = i\n",
        "\n",
        "  #sort dictionary by value (first the more reccurrent ones)\n",
        "  for i in dict(sorted(entities.items(), key=lambda item: item[1], reverse = True)):\n",
        "    if \"\\n\" in i: #if there are '\\n' delete them\n",
        "      words = i.split('\\n')\n",
        "      for k in words: #check the substring obtained\n",
        "        if len(k.split(' ')) > 1:\n",
        "          people.append(k)\n",
        "    else: #if there isn't any '\\n' check, the entity\n",
        "      if len(i.split(' ')) > 1:\n",
        "        people.append(i)\n",
        "\n",
        "  return time_word, people\n",
        "\n"
      ],
      "metadata": {
        "id": "TzvhJ68aFqeG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merge keybert and ner results (removing pronouns and adding entities, and adding time in front)\n",
        "def merge_data(phrases, time, entities):\n",
        "  to_be = [\"i'm\", \"i am\", \"you're\", \"you are\", \"he's\", \"he is\", \"she's\", \"she is\"]\n",
        "  to_have = [\"i've\", \"i have\", \"you've\", \"you have\", \"he has\", \"she has\"]\n",
        "  possession_act = [\"my \", \"your \", \"his \", \"her \"]\n",
        "  third_person = [\"she \", \"he \"]\n",
        "  replaced = []\n",
        "  results = []\n",
        "\n",
        "  #case in which no entities are detected\n",
        "  if len(time) <= 0 and len(entities) <= 0:\n",
        "    for i in phrases:\n",
        "      results.append(i.capitalize())\n",
        "    return results\n",
        "\n",
        "  #case in which entities are detected\n",
        "  if len(entities) > 0:\n",
        "    for i in phrases:\n",
        "      i = i.lower()\n",
        "      c = 0\n",
        "\n",
        "      #check to_be\n",
        "      for j in to_be:\n",
        "        if j in i:\n",
        "          if len(entities) > c:\n",
        "            i = i.replace(j, entities[c] + \" is\")\n",
        "            c += 1\n",
        "          else: break\n",
        "\n",
        "      #check to_have\n",
        "      for j in to_have:\n",
        "        if j in i:\n",
        "          if len(entities) > c:\n",
        "            i = i.replace(j, entities[c] + \" has\")\n",
        "            c += 1\n",
        "          else: break\n",
        "\n",
        "      #check possession_act\n",
        "      for j in possession_act:\n",
        "        if j in i:\n",
        "          if len(entities) > c:\n",
        "            i = i.replace(j, entities[c] + \" \" )\n",
        "            c += 1\n",
        "          else: break\n",
        "\n",
        "      #check possession_pass\n",
        "      for j in third_person:\n",
        "        if j in i:\n",
        "          if len(entities) > c:\n",
        "            i = i.replace(j, entities[c] + \" \" )\n",
        "            c += 1\n",
        "          else: break\n",
        "\n",
        "      #if something change\n",
        "      if c != 0:\n",
        "        replaced.append(i)\n",
        "\n",
        "  #case in which time is detected\n",
        "  if len(time) > 0:\n",
        "    #case in which also entities are detected\n",
        "    if len(replaced) > 0:\n",
        "      #merge all\n",
        "      time = time.capitalize()\n",
        "      for i in replaced:\n",
        "        results.append(time + \" \" + i)\n",
        "    #case in which no entities are detected\n",
        "    else:\n",
        "      time = time.capitalize()\n",
        "      for i in phrases:\n",
        "        results.append(time + \" \" + i)\n",
        "  else:\n",
        "     for i in phrases:\n",
        "      results.append(i.capitalize())\n",
        "\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "9YJzf7vnEtyF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to generate text (using gpt2 finetuned)\n",
        "def generate_story(gpt2, tokenizer_gpt2, start, k=0, p=0.9, output_length=300, temperature=1, num_return_sequences=2, repetition_penalty=1.1):\n",
        "  texts = []\n",
        "  encoded_start = tokenizer_gpt2.encode(start, padding=True,truncation=True, max_length=512, return_tensors = \"pt\")\n",
        "  output_sequences = gpt2.generate(input_ids = encoded_start, max_new_tokens = output_length, temperature = temperature, top_k = k, top_p = p, repetition_penalty = repetition_penalty, do_sample = True, num_return_sequences = num_return_sequences)\n",
        "  if len(output_sequences.shape) > 2:\n",
        "    output_sequences.squeeze_()\n",
        "  for _ , generated_sequence in enumerate(output_sequences):\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "    # Decode text\n",
        "    text = tokenizer_gpt2.decode(generated_sequence, clean_up_tokenization_spaces = True)\n",
        "    # Remove all text after eos token\n",
        "    text = text[: text.find(tokenizer_gpt2.eos_token)]\n",
        "    #print(text, \"\\n\")\n",
        "    texts.append(text)\n",
        "  return texts"
      ],
      "metadata": {
        "id": "O_5sI27nnzKF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to correct the grammar (using T5 finetuned)\n",
        "def grammar_correction(t5, tokenizer_t5, input_text):\n",
        "  # Tokenize input text\n",
        "  input_text = \"correcting: \" + input_text\n",
        "  input_ids = tokenizer_t5.encode(input_text, return_tensors='pt')\n",
        "  output = t5.generate(input_ids=input_ids)\n",
        "  output_text = tokenizer_t5.decode(output[0], skip_special_tokens=True)\n",
        "  return output_text"
      ],
      "metadata": {
        "id": "q2wNa7MmpcWt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate perplexity with nltk\n",
        "def calculate_perplexity(generated_text, train_frac=0.8, n=2):\n",
        "    tokens = nltk.word_tokenize(generated_text.lower())\n",
        "    split_idx = int(train_frac * len(tokens))\n",
        "    train_tokens = tokens[:split_idx]\n",
        "    test_tokens = tokens[split_idx:]\n",
        "\n",
        "    ngrams, vocab = padded_everygram_pipeline(n, train_tokens)\n",
        "    lm = Laplace(n)\n",
        "    lm.fit(ngrams, vocab)\n",
        "    test_ngrams = list(nltk.everygrams(test_tokens, max_len=n))\n",
        "    perplexity = lm.perplexity(test_ngrams)\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "8P9odrxvuMII"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation indices for the texts geenrated\n",
        "def evaluation_text(texts):\n",
        "  #first\n",
        "  first = []\n",
        "  text_generated = texts[0]\n",
        "  first.append(textstat.flesch_kincaid_grade(text_generated)) #Flesch-Kincaid Grade Level: This index estimates the grade level required to understand the text.\n",
        "  first.append(textstat.automated_readability_index(text_generated))  #Automated Readability Index (ARI): The ARI measures the readability of a text based on characters per word and words per sentence.\n",
        "  first.append(textstat.coleman_liau_index(text_generated))  #Coleman-Liau Index: This index uses characters per word and words per sentence to estimate the grade level required to understand the text\n",
        "  first.append(textstat.gunning_fog(text_generated))  #Gunning Fog Index: The Gunning Fog Index estimates the years of formal education needed to understand the text.\n",
        "  first.append(textstat.smog_index(text_generated))  #SMOG Index: The SMOG index estimates the grade level required to understand the text based on the number of polysyllabic words.\n",
        "  first.append(textstat.flesch_reading_ease(text_generated))\n",
        "  first.append(calculate_perplexity(text_generated, train_frac=0.8, n=2))\n",
        "\n",
        "  #second\n",
        "  second = []\n",
        "  text_generated = texts[1]\n",
        "  second.append(textstat.flesch_kincaid_grade(text_generated)) #Flesch-Kincaid Grade Level: This index estimates the grade level required to understand the text.\n",
        "  second.append(textstat.automated_readability_index(text_generated))  #Automated Readability Index (ARI): The ARI measures the readability of a text based on characters per word and words per sentence.\n",
        "  second.append(textstat.coleman_liau_index(text_generated))  #Coleman-Liau Index: This index uses characters per word and words per sentence to estimate the grade level required to understand the text\n",
        "  second.append(textstat.gunning_fog(text_generated))  #Gunning Fog Index: The Gunning Fog Index estimates the years of formal education needed to understand the text.\n",
        "  second.append(textstat.smog_index(text_generated))  #SMOG Index: The SMOG index estimates the grade level required to understand the text based on the number of polysyllabic words.\n",
        "  second.append(textstat.flesch_reading_ease(text_generated))\n",
        "  second.append(calculate_perplexity(text_generated, train_frac=0.8, n=2))\n",
        "\n",
        "  #choose\n",
        "  score1 = 0\n",
        "  score2 = 0\n",
        "  res = []\n",
        "  for i in range(0, len(first)):\n",
        "    #because the 5th indice is better is is higher\n",
        "    if i in [5]:\n",
        "      if first[i] > second[i]:\n",
        "        score1 += 1\n",
        "      else:\n",
        "        score2 += 1\n",
        "    else:\n",
        "      if first[i] > second[i]:\n",
        "        score2 += 1\n",
        "      else:\n",
        "        score2 += 1\n",
        "\n",
        "  #end\n",
        "  if score2 > score1:\n",
        "    return 1, second\n",
        "  else:\n",
        "    return 0, first\n"
      ],
      "metadata": {
        "id": "GVnCdXJnsVuw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#choose best between the two candidate (already calcolated the scores)\n",
        "def choose_best(first, second, txt1, txt2):\n",
        "  #choose\n",
        "  score1 = 0\n",
        "  score2 = 0\n",
        "  res = []\n",
        "  for i in range(0, len(first)):\n",
        "    #because the 5th indice is better is is higher\n",
        "    if i in [5]:\n",
        "      if first[i] > second[i]:\n",
        "        score1 += 1\n",
        "      else:\n",
        "        score2 += 1\n",
        "    else:\n",
        "      if first[i] > second[i]:\n",
        "        score2 += 1\n",
        "      else:\n",
        "        score2 += 1\n",
        "\n",
        "  if score2 > score1:\n",
        "    return second, txt2\n",
        "  else:\n",
        "    return first, txt1"
      ],
      "metadata": {
        "id": "xxriy1DQ3KVg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print information andscores about the text selected as the best one generated\n",
        "def final_print(scores, txt):\n",
        "  #unique words\n",
        "  unique_words = set(txt.split())\n",
        "  num_unique_words = len(unique_words)\n",
        "\n",
        "  #avg sentence length (in words)\n",
        "  sents = txt.split('.')\n",
        "  avg_sentence_len_words = sum(len(x.split()) for x in sents) / len(sents)\n",
        "\n",
        "  #avg words and sentences length (in characters)\n",
        "  words = txt.split()\n",
        "  sentences = txt.split(\".\")\n",
        "  total_words = 0\n",
        "  total_sentences = 0\n",
        "\n",
        "  for word in words:\n",
        "      total_words += len(word)\n",
        "\n",
        "  for sentence in sentences:\n",
        "      total_sentences += len(sentence)\n",
        "  average_word_length = total_words / len(words)\n",
        "  average_sentence_length = total_sentences / len(sentences)\n",
        "\n",
        "  print(\"__________________________________________________________________________________\")\n",
        "  print(\"METRICS\")\n",
        "  print(\"grade_level: \", scores[0])\n",
        "  print(\"ari: \", scores[1])\n",
        "  print(\"coleman_liau: \", scores[2])\n",
        "  print(\"gunning_fog: \", scores[3])\n",
        "  print(\"smog: \", scores[4])\n",
        "  print(\"reading_ease_score: \", scores[5])\n",
        "  print(\"perplexity_score: \", scores[6])\n",
        "  print(\"DATA\")\n",
        "  print(\"number unique words used: \", num_unique_words)\n",
        "  print(\"average words length in char: \", average_word_length)\n",
        "  print(\"avg sentences length in words: \", avg_sentence_len_words)\n",
        "  print(\"average sentences length in char: \", average_sentence_length)\n",
        "  print(\"__________________________________________________________________________________\")\n",
        "  print(\"TEXT OUTPUT\")\n",
        "  print(txt)\n",
        "  print(\"__________________________________________________________________________________\")"
      ],
      "metadata": {
        "id": "g0I_3qRF3mBf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#try to delete incomplete phrases at the end\n",
        "def redefine(txt):\n",
        "  res = \"\"\n",
        "  for c, i in enumerate(txt):\n",
        "    if i == \"\\n\" and len(txt[c:]) < 30: #delte the last characters only if they are less than 30 (otherwise I delete too much text)\n",
        "      res = txt[:c]\n",
        "  if res == \"\":\n",
        "    return txt\n",
        "  else:\n",
        "    return res"
      ],
      "metadata": {
        "id": "At8WAbQJ9MAi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main section"
      ],
      "metadata": {
        "id": "P3fUKfAgEh9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pipeline function (generate stroies from lyrics)\n",
        "def Stories_from_lyrics(lyrics_path, keybert, ner_spacy, t5, gpt2, tokenizer_t5, tokenizer_gpt2):\n",
        "  #variables\n",
        "  n_span = []\n",
        "  entities = {}\n",
        "  time = []\n",
        "  time_word = \"\"\n",
        "  max = -1\n",
        "  people = []\n",
        "\n",
        "  #open lyrics\n",
        "  f = open(lyrics_path, \"r\")\n",
        "  lyrics = f.read()\n",
        "\n",
        "  #KEYBERT\n",
        "  keywords = keybert.extract_keywords(lyrics, keyphrase_ngram_range=(1, 5), stop_words=None, use_maxsum=True, nr_candidates=20, top_n=2)\n",
        "  for i in range(0, len(keywords)): #convert keywords extracted into string array\n",
        "    n_span.append(keywords[i][0])\n",
        "\n",
        "  #SPACY NER\n",
        "  ner_result = ner(lyrics) #do ner\n",
        "  time_word, people = elaborate_ner_results(ner_result) #elaborate results\n",
        "\n",
        "\n",
        "  print(\"ner: \", people, \" and time: \", time_word)\n",
        "\n",
        "  #MERGE NER AND SPAN\n",
        "  res = merge_data(n_span, time_word, people)\n",
        "\n",
        "  print(\"res merg: \", res)\n",
        "\n",
        "  #GRAMMAR CORRECTION\n",
        "  corrected = []\n",
        "  for i in res:\n",
        "    corrected.append(grammar_correction(t5, tokenizer_t5, i))\n",
        "\n",
        "  #GPT2\n",
        "  output1 = generate_story(gpt2, tokenizer_gpt2, corrected[0])\n",
        "  output2 = generate_story(gpt2, tokenizer_gpt2, corrected[1])\n",
        "\n",
        "\n",
        "  #BEST TEXT CHOOSE\n",
        "  index_best1, scores_best1 = evaluation_text(output1)\n",
        "  index_best2, scores_best2 = evaluation_text(output2)\n",
        "  #find best option between the two selected\n",
        "  final_result_scores, final_result_text = choose_best(scores_best1, scores_best2, output1[index_best1], output2[index_best2])\n",
        "  #cut final part not finished\n",
        "  final_result_text = redefine(final_result_text)\n",
        "  #print information\n",
        "  final_print(final_result_scores, final_result_text)\n",
        "\n",
        "\n",
        "  return final_result_text\n",
        "\n"
      ],
      "metadata": {
        "id": "A37jLz8KAFmI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define paths\n",
        "\n",
        "#path_gpt2 = \"story_gpt2_2epochs.pt\"\n",
        "#path_gpt2 = \"tinystory_gpt2_1epoch.pt\"\n",
        "path_gpt2 = \"tinystory_gpt2_1epoch_v80.pt\"\n",
        "\n",
        "path_t5 = \"t5_gc.pt\"\n",
        "\n",
        "#lyrics_path = \"withoutme.txt\"\n",
        "#lyrics_path = \"moves_like_jagger.txt\"\n",
        "#lyrics_path = \"levitating.txt\"\n",
        "#lyrics_path = \"party_in_the_usa.txt\"\n",
        "#lyrics_path = \"flowers.txt\"\n",
        "lyrics_path = \"perfect.txt\"\n",
        "#lyrics_path = \"Empire_state_of_mind.txt\""
      ],
      "metadata": {
        "id": "kSx8E1QDiskZ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD MODELS\n",
        "\n",
        "#define device\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token\n",
        "\n",
        "#define tokenizer\n",
        "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "tokenizer_t5.pad_token = tokenizer_t5.eos_token\n",
        "\n",
        "\n",
        "keybert = KeyBERT()#.to(device)\n",
        "ner = spacy.load('en_core_web_sm')#.to(device)\n",
        "#gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "gpt2 = torch.load(path_gpt2, map_location=torch.device(device))\n",
        "t5 = torch.load(path_t5, map_location=torch.device(device))\n",
        "gpt2.eval()\n",
        "t5.eval()\n",
        "#generator = pipeline('text-generation', model=\"facebook/opt-350m\", do_sample=True, num_return_sequences=5).to(device)"
      ],
      "metadata": {
        "id": "gkQQ-_PuioSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = Stories_from_lyrics(lyrics_path, keybert, ner, t5, gpt2, tokenizer_t5, tokenizer_gpt2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfeVJGY8ibSt",
        "outputId": "a180cee7-7931-414d-cab4-2ff3c0c9cb54"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ner:  []  and time:  \n",
            "res merg:  ['You re holding mine baby', 'Found girl beautiful and sweet']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________________________________________________________________________________\n",
            "METRICS\n",
            "grade_level:  5.0\n",
            "ari:  6.0\n",
            "coleman_liau:  6.89\n",
            "gunning_fog:  6.33\n",
            "smog:  8.2\n",
            "reading_ease_score:  83.15\n",
            "perplexity_score:  190.52279850834444\n",
            "DATA\n",
            "number unique words used:  168\n",
            "average words length in char:  4.389105058365759\n",
            "avg sentences length in words:  15.117647058823529\n",
            "average sentences length in char:  81.0\n",
            "__________________________________________________________________________________\n",
            "TEXT OUTPUT\n",
            "Found girl beautiful and sweet, she had a secret. She always wanted to have fun with her friends. One day she saw a room filled with pictures and games. She felt sad for not having enough space to explore them all.\n",
            "\n",
            "Her friend, a boy named Jack the bear then smiled and said that it was good to spend time together and be happy. He asked if they could both play some games like this one. He pointed to a movie about dinosaurs and winked. The kids listened eagerly, and agreed to join in.\n",
            " \n",
            "After playing, the children continued to have lots of fun together until it was time for lunchtime. All of them were hungry and full of energy. They decided to run around the room before going out and having fun, even when they had very little time. \n",
            "\n",
            "The house had a nice sunny afternoon, filled with the sunshine. They were exhausted and happy. From that day on whenever they got tired and wanted to take a nap, they would still go to the room, feeling well rested and refreshed. They forgot all about their empty space and enjoyed the rest of the afternoon. The end! \n",
            "\n",
            "The End! \n",
            "\n",
            "The girls grinned and hugged each other as they walked home, happy and full from the wonderful moment they spent together at lunchtime. They knew they would come back many more times after that day! And so, keep reading for more mysteries and secrets waiting to hear! The End! And remember to enjoy your time with friend\n",
            "__________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cU1yFgu68QUE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}